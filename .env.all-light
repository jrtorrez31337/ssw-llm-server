# SSW AI Inference Platform — All-Light Configuration
# 10× Qwen3-4B-AWQ workers (5 per GPU), no heavy workers
#
# Usage: docker compose -f docker-compose.all-light.yml --env-file .env.all-light up -d

# === Model Paths (mounted from host) ===
MODEL_REPO=/data/models
LIGHT_MODEL=Qwen/Qwen3-4B-AWQ

# === vLLM Settings ===
LIGHT_GPU_MEMORY_UTILIZATION=0.17
VLLM_MAX_MODEL_LEN=16384
VLLM_TOOL_CALL_PARSER=hermes

# === Worker Ports (host-mapped) ===
LIGHT_0_PORT=8001
LIGHT_1_PORT=8002
LIGHT_2_PORT=8003
LIGHT_3_PORT=8004
LIGHT_4_PORT=8005
LIGHT_5_PORT=8006
LIGHT_6_PORT=8007
LIGHT_7_PORT=8008
LIGHT_8_PORT=8009
LIGHT_9_PORT=8010

# === Redis ===
REDIS_PORT=6379

# === Gateway ===
GATEWAY_PORT=8000

# === Loader ===
LOADER_PORT=8011
